<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Improving robot navigation through semantic image segmentation</title>
    <meta name="generator" content="VuePress 1.4.1">
    
    <meta name="description" content=" ">
    <link rel="preload" href="/groundsegmentation/assets/css/0.styles.0f6c4cf6.css" as="style"><link rel="preload" href="/groundsegmentation/assets/js/app.5164ea9a.js" as="script"><link rel="preload" href="/groundsegmentation/assets/js/2.b39596c5.js" as="script"><link rel="preload" href="/groundsegmentation/assets/js/5.7fbf407f.js" as="script"><link rel="prefetch" href="/groundsegmentation/assets/js/3.80eba41b.js"><link rel="prefetch" href="/groundsegmentation/assets/js/4.fca74266.js"><link rel="prefetch" href="/groundsegmentation/assets/js/6.69a159d7.js">
    <link rel="stylesheet" href="/groundsegmentation/assets/css/0.styles.0f6c4cf6.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-navbar no-sidebar"><!----> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <!----> </aside> <main aria-labelledby="main-title" class="home"><header class="hero"><!----> <h1 id="main-title">
      Improving robot navigation through semantic image segmentation
    </h1> <p class="description">
       
    </p> <p class="action"><a href="https://github.com/rettier/groundsegmentation/" target="_blank" rel="noopener noreferrer" class="nav-link external action-button">
  Get the sourcecode →
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></header> <!----> <div class="theme-default-content custom content__default"><h2 id="introduction"><a href="#introduction" class="header-anchor">#</a> Introduction</h2> <p>This repository contains source code for the ground segmentation described in my <a href="improving-robot-navigation-through-semantic-image-segmentation.pdf">thesis</a>.</p> <img width="50%" src="result.png"> <p>The data generation described in the thesis  was implemented in Python3 with the help of open source libraries. The pipeline is split into individual steps. Each step is executed individually via separate python files.</p> <p>Each step is written to only process new data and skip already processed files. This allows you to simply run the whole pipeline again after creating new recordings files. No parameters need to be passed specifically for each recording. To reset or rerun the pipeline for a specific dataset, simply delete its corresponding folder in the configured data output folder and re-run the pipeline.</p> <h2 id="dependencies"><a href="#dependencies" class="header-anchor">#</a> Dependencies</h2> <p>The project depends on certain python libraries to be present, the most important ones are described here. A full list of dependencies can be found in the included <code>requirements.txt</code> file.</p> <ul><li>The <strong>rosbag</strong> package is used to read the rosbag recording files. It is required for step 1 of the process. This library is automatically present if ros is installed or can be installed via the python package manager.</li> <li><strong>OpenCV</strong> is used for common image manipulation tasks and the point cloud computation. Its implementation of the Stereo Block Matching Algorithm was used to convert the two infrared images into a disparity map. Further, the projection from the disparity map into the 3d point cloud and the projection from the 3d point cloud into the color image frame were sped up with the help of opencv's native implementation.</li> <li><strong>python-pcl</strong>: The calculation of the normal vectors of the point cloud points and the plane segmentation with RANSAC was done using the point cloud library. To access the point cloud library functions in python the python-pcl wrapper was used. It allows us to access some of its functions from python.</li> <li><strong>PyDenseCRF</strong> is a python wrapper for Philipp Krähenbühl's dense (fully connected) CRFs. It is used for the mask refinement in step 5.</li></ul> <h2 id="configuration"><a href="#configuration" class="header-anchor">#</a> Configuration</h2> <p>The configuration file <code>config.py</code> defines the source folder of the recordings (rosbag files) and also the destination folder of the output files. Furthermore, thresholds and parameters for various computation steps can be configured. The configuration file contains comments about what effect the individual parameters have. The default configuration works well for the used Intel RealSense D435.</p> <h3 id="inverted-mode"><a href="#inverted-mode" class="header-anchor">#</a> Inverted mode</h3> <p>If the camera was mounted upside-down, the dataset can be processed as-well without any changes to the code by changing the dataset's name to end in <em>_inv</em>. This is useful when the camera is mounted upside down on the robot or a second recording is made with the camera upside down to avoid problems with occlusion as discussed in the thesis.</p> <h2 id="step-1-data-extraction"><a href="#step-1-data-extraction" class="header-anchor">#</a> Step 1: Data extraction</h2> <p>This script extracts the images and extrinsic and intrinsic camera parameters from the rosbag file. It expects a certain format in which the files have to be in the configured input folder. Each recording session has its own subfolder, in which rosbag records the individual files one by one. The name of the final dataset will be the name of the folder. The project includes a utility script <code>record.sh</code> which automatically creates a folder and launches rosbag with the correct parameters.</p> <div class="language- extra-class"><pre class="language-text"><code>recordings/
  - indoor1/
    - indoor1_2000-01-02-15-42-52_0.bag
    - indoor1_2000-01-02-15-42-52_1.bag
  - indoor2/
    - indoor2_2000-01-02-13-22-12_0.bag
    ...
</code></pre></div><p>Step 1 produces the following output files per chunk of each recording.</p> <ul><li><strong>color.npy</strong> all color images as numpy matrix of type <code>uint8</code> and shape <code>N x W_c x H_c x 3</code>.</li> <li><strong>infra1.npy, infra2.npy</strong> the left and right infrared images as a numpy matrix of shape <code>N x W_i x H_i x 1</code>.</li></ul> <p>Where <code>N</code> is the number of output recorded frames, <code>H_c</code>, <code>W_c</code> is the resolution of the color image and <code>H_i</code>, <code>W_i</code> the resolution of the infrared images.</p> <h3 id="frame-synchronization"><a href="#frame-synchronization" class="header-anchor">#</a> Frame synchronization</h3> <p>The script expects both infrared frames and the color frame to be in sync. This is enforced by only extracting IR and color frames where the timestamp of the message matches exactly.</p> <p>This was found to be the case for the Intel RealSense D435, as long as the exposure of the color camera is lower than one over the infrared camera's framerate. If the color camera's exposure is set to automatic, frames may be skipped in low lighting conditions. The output of the script shows how many matching and non-matching frames were found.</p> <div class="language- extra-class"><pre class="language-text"><code>$ python step1_bagfile.py
looking for bagfiles in /home/user/Documents/
extracting /home/user/Documents/demo/2020-01-03-12-02-23_1.bag
found 192 matching pairs of 237 total frames
extracting /home/user/Documents/demo/2020-01-03-12-02-13_0.bag
found 210 matching pairs of 236 total frames
extracting /home/user/Documents/demo/2020-01-03-12-02-33_2.bag
found 72 matching pairs of 97 total frames
</code></pre></div><h2 id="step-2-data-reduction"><a href="#step-2-data-reduction" class="header-anchor">#</a> Step 2: Data reduction</h2> <p>This step reduces the recorded frames based on the technique described in the thesis. The threshold for the dissimilarity measure and the batch size (<code>N_{batch}</code>) from which the sharpest image is selected can be set in the configuration file. The data reduction is performed for each chunk of the recording individually. After all chunks of a recording are reduced they are combined into one file and the temporary files from each chunk are deleted.</p> <p>The batch size, from which the sharpest frame will be drawn, dictates the maximum rate at which new frames will be chosen and saved in the final output. This rate can be calculated by <code>FPS_{max} = FPS_{recording} / N_{batch}</code></p> <p>where $FPS_{recording}$ is the frame rate at which the camera recorded. Additionally, if the camera image was still during parts of the recording, the final output frame rate may be lower.</p> <p>Step 2 produces the same output files as step 1, but with reduced data and all chunks of a recording will be combined into one output file. The step's output lists all processed steps.</p> <div class="language- extra-class"><pre class="language-text"><code>$ python step2_reduce_data.py
reduced /data/demo_2020-01-03-12-02-23_1 from 192 frames to 14
reduced /data/demo_2020-01-03-12-02-13_0 from 210 frames to 13
reduced /data/demo_2020-01-03-12-02-33_2 from 72 frames to 3
combining /data/demo
deleting /data/demo_2020-01-03-12-02-23_1
deleting /data/demo_2020-01-03-12-02-13_0
deleting /data/demo_2020-01-03-12-02-33_2
</code></pre></div><h2 id="step-3-point-cloud-calculation"><a href="#step-3-point-cloud-calculation" class="header-anchor">#</a> Step 3: Point cloud calculation</h2> <p>This step takes the previously extracted infrared images from <code>infra1.npy</code> and  <code>infra2.npy</code> and calculates a 3d point cloud without any color information. This point cloud is saved to <code>coords.npy</code> as a numpy array with type <code>float32</code> and shape <code>N x (W_i * H_i) x 3</code>. Any points without disparity information or out of plausible range are filled with <code>NaN</code> values.</p> <p>The parameters for the Stereo Block Matching algorithm can be changed in the configuration file. The extrinsic and intrinsic camera parameters for the projection into 3d are taken from the data camera parameters which are published on the ROS camera topic and the ROS transform system. For this step to work properly the camera has to be properly setup in ROS and defined in the transformation graph.</p> <h2 id="step-4-plane-segmentation"><a href="#step-4-plane-segmentation" class="header-anchor">#</a> Step 4: plane segmentation</h2> <p>Step 4 uses the calculated point cloud from the previous step and extracts the ground plane. The calculated plane's model is saved to <code>planes.npy</code> .
Since the segmentation is not only based on the distance from the ground plane, but additional constraints, a map of all inliers is saved in <code>good_idx.npy</code> as a numpy array with type <code>bool</code> and shape <code>N x (W_i * H_i) x 1</code>.</p> <h2 id="step-5-mask-generation"><a href="#step-5-mask-generation" class="header-anchor">#</a> Step 5: mask generation</h2> <p>In this step, the calculated inliers are projected into the color camera's reference frame. The initial mask is refined as described in the thesis. The refined mask and the color images are saved into individual folders in the the output folder.
The masks are saved as indexed png files. Where index 0 is the ground class, index 1 is the obstacle class and index 255 is the ignore class.</p> <div class="language- extra-class"><pre class="language-text"><code>output/
  - indoor1/
    - color/
      indoor1_00000.png
      indoor1_00001.png
      ...
    - label/
      indoor1_00000.png
      indoor1_00001.png
      ...
</code></pre></div><h2 id="step-6-mask-refinement"><a href="#step-6-mask-refinement" class="header-anchor">#</a> Step 6: Mask refinement</h2> <p>This step implements the connected component refinement described in the thesis. The cleaned masks are saved analog to the input masks in an adjacent folder named <code>label_clean</code>.</p> <h2 id="step-7-manual-review"><a href="#step-7-manual-review" class="header-anchor">#</a> Step 7: Manual review</h2> <p>The last step in the pipeline is the human review step. A user interface was implemented to assist the reviewer in manually flag each frame as good or bad. The reviewer is presented with a blended image of a mask and the color image. He then has to decide whether to keep the frame or remove it. Additionally, a frame can be skipped to be reviewed later.</p> <p>Keyboard actions:</p> <ul><li><strong>Return</strong>: mark mask as good</li> <li><strong>Backspace</strong>: mark mask as bad</li> <li><strong>Left Arrow / Right Arrow</strong>: go one mask backwards / forwards</li> <li><strong>Escape</strong>: close the interface and save all changes</li> <li><strong>1, 2, 3</strong>: switch between (1) blended, (2) mask only or (3) color only view mode</li></ul> <p>The user interface outputs two text files in the output folder named <code>good.txt</code> and <code>bad.txt</code> with the respective file paths of good and bad masks. These files can then be used to collect the data for training the neuronal network.</p></div> <div class="footer">
    Philipp Reitter - 2020
  </div></main></div><div class="global-ui"></div></div>
    <script src="/groundsegmentation/assets/js/app.5164ea9a.js" defer></script><script src="/groundsegmentation/assets/js/2.b39596c5.js" defer></script><script src="/groundsegmentation/assets/js/5.7fbf407f.js" defer></script>
  </body>
</html>
